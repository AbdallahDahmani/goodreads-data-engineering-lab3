{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f6735f11-2212-4a09-ad0c-55e952d27a9f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 19688803 records\n+--------------------+--------+--------------------+---------+---------------+--------------------+------+--------------------+\n|           review_id| book_id|               title|author_id|           name|             user_id|rating|         review_text|\n+--------------------+--------+--------------------+---------+---------------+--------------------+------+--------------------+\n|165cf8dcbcf493129...|20172134|Nightmares! (Nigh...|   109354| Kirsten Miller|5ef3b7a0f64ae79d0...|     4|I was pleasantly ...|\n|2cd8eec721eaf9bee...| 7937843|                Room|    23613|  Emma Donoghue|5ccf302a3b317983e...|     2|I tend to have ha...|\n|b9fc663c5c884bdb7...|  341336|To the Edge (The ...|   195778|   Cindy Gerard|58f7d3af14dfa25ac...|     3|3 STARS \\n Jillia...|\n|3939f67856c8c1aaf...|18335634|Clockwork Princes...|   150038|Cassandra Clare|03a3c18b7d1da1c21...|     5|Warning: Emotiona...|\n|0efcd32371b30497c...|18304774|     Một cuộc gặp gỡ|     6343|  Milan Kundera|5ae5183d9cd8ca0d6...|     3|Khong hieu do ngu...|\n+--------------------+--------+--------------------+---------+---------------+--------------------+------+--------------------+\nonly showing top 5 rows\n"
     ]
    }
   ],
   "source": [
    "# Set up storage authentication (use your actual storage account and key)\n",
    "spark.conf.set(\n",
    "    \"fs.azure.account.key.goodreadsreviews60301511.dfs.core.windows.net\",\n",
    "    \"QZzPMlZcQvM/LeucwJ67H1zRkEhbWCH9+uxdJaTWALJU/QN8ArtpEhMHmVb7vT2DaXAgMY52PkPH+AStsA7+fw==\"\n",
    ")\n",
    "\n",
    "# Load the curated gold dataset\n",
    "curated_reviews_gold = spark.read.format(\"delta\").load(\n",
    "    \"abfss://lakehouse@goodreadsreviews60301511.dfs.core.windows.net/gold/curated_reviews/\"\n",
    ")\n",
    "\n",
    "print(f\"Loaded {curated_reviews_gold.count()} records\")\n",
    "curated_reviews_gold.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "238446ae-5387-4a6e-9110-00e270e10767",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: 13783384 records\nValidation: 2951447 records\nTest: 2953972 records\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import rand\n",
    "\n",
    "# Split the data into train, validation, and test sets\n",
    "splits = curated_reviews_gold.randomSplit([0.7, 0.15, 0.15], seed=42)\n",
    "train_df, val_df, test_df = splits\n",
    "\n",
    "print(f\"Train: {train_df.count()} records\")\n",
    "print(f\"Validation: {val_df.count()} records\")\n",
    "print(f\"Test: {test_df.count()} records\")\n",
    "\n",
    "# Save the splits to the gold layer\n",
    "train_df.write.mode(\"overwrite\").format(\"delta\").save(\"abfss://lakehouse@goodreadsreviews60301511.dfs.core.windows.net/gold/feature_v2/train\")\n",
    "val_df.write.mode(\"overwrite\").format(\"delta\").save(\"abfss://lakehouse@goodreadsreviews60301511.dfs.core.windows.net/gold/feature_v2/validation\")\n",
    "test_df.write.mode(\"overwrite\").format(\"delta\").save(\"abfss://lakehouse@goodreadsreviews60301511.dfs.core.windows.net/gold/feature_v2/test\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3a68bfd7-3ee0-41ff-b145-95a6c0d4a4dd",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql.types import *\n",
    "import re\n",
    "\n",
    "def clean_text(df, text_column=\"review_text\"):\n",
    "    \"\"\"\n",
    "    Comprehensive text cleaning function\n",
    "    \"\"\"\n",
    "    # Convert to lowercase\n",
    "    df = df.withColumn(text_column, lower(col(text_column)))\n",
    "    \n",
    "    # Remove extra whitespace\n",
    "    df = df.withColumn(text_column, regexp_replace(col(text_column), \"\\\\s+\", \" \"))\n",
    "    \n",
    "    # Remove punctuation (keep basic sentence structure)\n",
    "    df = df.withColumn(text_column, regexp_replace(col(text_column), \"[^a-zA-Z0-9\\\\s.!?]\", \"\"))\n",
    "    \n",
    "    # Trim whitespace\n",
    "    df = df.withColumn(text_column, trim(col(text_column)))\n",
    "    \n",
    "    # Replace URLs with placeholder\n",
    "    df = df.withColumn(text_column, regexp_replace(col(text_column), \"http\\\\S+\", \"<URL>\"))\n",
    "    \n",
    "    # Replace numbers with placeholder (optional - depends on your use case)\n",
    "    # df = df.withColumn(text_column, regexp_replace(col(text_column), \"\\\\d+\", \"<NUM>\"))\n",
    "    \n",
    "    return df\n",
    "\n",
    "# Apply cleaning to train, validation, and test sets\n",
    "train_df_clean = clean_text(train_df)\n",
    "val_df_clean = clean_text(val_df)\n",
    "test_df_clean = clean_text(test_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0de3d309-a8a3-421d-aab1-71f677d74de1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Basic features added:\n+-------------------+-------------------+--------------+-----------------+\n|review_length_words|review_length_chars|sentence_count|  avg_word_length|\n+-------------------+-------------------+--------------+-----------------+\n|                 56|                303|             5|5.410714285714286|\n|                 22|                119|             5|5.409090909090909|\n|                120|                654|             7|             5.45|\n|                 84|                443|             3|5.273809523809524|\n|                  3|                 28|             0|9.333333333333334|\n+-------------------+-------------------+--------------+-----------------+\nonly showing top 5 rows\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import size, split, length\n",
    "\n",
    "def extract_basic_features(df, text_column=\"review_text\"):\n",
    "    \"\"\"\n",
    "    Extract basic text features\n",
    "    \"\"\"\n",
    "    # Word count\n",
    "    df = df.withColumn(\"review_length_words\", size(split(col(text_column), \" \")))\n",
    "    \n",
    "    # Character count\n",
    "    df = df.withColumn(\"review_length_chars\", length(col(text_column)))\n",
    "    \n",
    "    # Sentence count (approximate)\n",
    "    df = df.withColumn(\"sentence_count\", \n",
    "                      size(split(col(text_column), \"[.!?]+\")) - 1)\n",
    "    \n",
    "    # Average word length\n",
    "    df = df.withColumn(\"avg_word_length\", \n",
    "                      col(\"review_length_chars\") / col(\"review_length_words\"))\n",
    "    \n",
    "    return df\n",
    "\n",
    "# Apply to all datasets\n",
    "train_df_features = extract_basic_features(train_df_clean)\n",
    "val_df_features = extract_basic_features(val_df_clean)\n",
    "test_df_features = extract_basic_features(test_df_clean)\n",
    "\n",
    "print(\"Basic features added:\")\n",
    "train_df_features.select(\"review_length_words\", \"review_length_chars\", \"sentence_count\", \"avg_word_length\").show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8e129257-6e7f-4d25-bed7-4b27313939eb",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: nltk in /databricks/python3/lib/python3.12/site-packages (3.8.1)\nCollecting textblob\n  Downloading textblob-0.19.0-py3-none-any.whl.metadata (4.4 kB)\nCollecting textstat\n  Downloading textstat-0.7.11-py3-none-any.whl.metadata (15 kB)\nRequirement already satisfied: click in /databricks/python3/lib/python3.12/site-packages (from nltk) (8.1.7)\nRequirement already satisfied: joblib in /databricks/python3/lib/python3.12/site-packages (from nltk) (1.4.2)\nRequirement already satisfied: regex>=2021.8.3 in /databricks/python3/lib/python3.12/site-packages (from nltk) (2023.10.3)\nRequirement already satisfied: tqdm in /databricks/python3/lib/python3.12/site-packages (from nltk) (4.66.4)\nCollecting nltk\n  Downloading nltk-3.9.2-py3-none-any.whl.metadata (3.2 kB)\nCollecting pyphen (from textstat)\n  Downloading pyphen-0.17.2-py3-none-any.whl.metadata (3.2 kB)\nRequirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from textstat) (74.0.0)\nDownloading textblob-0.19.0-py3-none-any.whl (624 kB)\n\u001B[?25l   \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m0.0/624.3 kB\u001B[0m \u001B[31m?\u001B[0m eta \u001B[36m-:--:--\u001B[0m\n\u001B[2K   \u001B[91m╸\u001B[0m\u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m10.2/624.3 kB\u001B[0m \u001B[31m?\u001B[0m eta \u001B[36m-:--:--\u001B[0m\n\u001B[2K   \u001B[91m━\u001B[0m\u001B[91m╸\u001B[0m\u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m30.7/624.3 kB\u001B[0m \u001B[31m958.0 kB/s\u001B[0m eta \u001B[36m0:00:01\u001B[0m\n\u001B[2K   \u001B[91m━━\u001B[0m\u001B[91m╸\u001B[0m\u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m41.0/624.3 kB\u001B[0m \u001B[31m382.3 kB/s\u001B[0m eta \u001B[36m0:00:02\u001B[0m\n\u001B[2K   \u001B[91m━━━━━\u001B[0m\u001B[91m╸\u001B[0m\u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m92.2/624.3 kB\u001B[0m \u001B[31m704.8 kB/s\u001B[0m eta \u001B[36m0:00:01\u001B[0m\n\u001B[2K   \u001B[91m━━━━━━━\u001B[0m\u001B[90m╺\u001B[0m\u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m112.6/624.3 kB\u001B[0m \u001B[31m813.1 kB/s\u001B[0m eta \u001B[36m0:00:01\u001B[0m\n\u001B[2K   \u001B[91m━━━━━━━━━━\u001B[0m\u001B[91m╸\u001B[0m\u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m174.1/624.3 kB\u001B[0m \u001B[31m857.3 kB/s\u001B[0m eta \u001B[36m0:00:01\u001B[0m\n\u001B[2K   \u001B[91m━━━━━━━━━━━━━━━━━\u001B[0m\u001B[91m╸\u001B[0m\u001B[90m━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m276.5/624.3 kB\u001B[0m \u001B[31m1.2 MB/s\u001B[0m eta \u001B[36m0:00:01\u001B[0m\n\u001B[2K   \u001B[91m━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[91m╸\u001B[0m\u001B[90m━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m307.2/624.3 kB\u001B[0m \u001B[31m1.1 MB/s\u001B[0m eta \u001B[36m0:00:01\u001B[0m\n\u001B[2K   \u001B[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[91m╸\u001B[0m\u001B[90m━━━━━━━━━\u001B[0m \u001B[32m481.3/624.3 kB\u001B[0m \u001B[31m1.6 MB/s\u001B[0m eta \u001B[36m0:00:01\u001B[0m\n\u001B[2K   \u001B[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[90m╺\u001B[0m\u001B[90m━━\u001B[0m \u001B[32m583.7/624.3 kB\u001B[0m \u001B[31m1.8 MB/s\u001B[0m eta \u001B[36m0:00:01\u001B[0m\n\u001B[2K   \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m624.3/624.3 kB\u001B[0m \u001B[31m1.7 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\n\u001B[?25hDownloading nltk-3.9.2-py3-none-any.whl (1.5 MB)\n\u001B[?25l   \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m0.0/1.5 MB\u001B[0m \u001B[31m?\u001B[0m eta \u001B[36m-:--:--\u001B[0m\n\u001B[2K   \u001B[91m━━━━━━━━━━━━━━\u001B[0m\u001B[90m╺\u001B[0m\u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m0.5/1.5 MB\u001B[0m \u001B[31m16.1 MB/s\u001B[0m eta \u001B[36m0:00:01\u001B[0m\n\u001B[2K   \u001B[91m━━━━━━━━━━━━━━━━━\u001B[0m\u001B[90m╺\u001B[0m\u001B[90m━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m0.6/1.5 MB\u001B[0m \u001B[31m15.8 MB/s\u001B[0m eta \u001B[36m0:00:01\u001B[0m\n\u001B[2K   \u001B[91m━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[91m╸\u001B[0m\u001B[90m━━━━━━━━━━━━━━━\u001B[0m \u001B[32m0.9/1.5 MB\u001B[0m \u001B[31m9.1 MB/s\u001B[0m eta \u001B[36m0:00:01\u001B[0m\n\u001B[2K   \u001B[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[91m╸\u001B[0m \u001B[32m1.5/1.5 MB\u001B[0m \u001B[31m11.8 MB/s\u001B[0m eta \u001B[36m0:00:01\u001B[0m\n\u001B[2K   \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m1.5/1.5 MB\u001B[0m \u001B[31m10.6 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\n\u001B[?25hDownloading textstat-0.7.11-py3-none-any.whl (176 kB)\n\u001B[?25l   \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m0.0/176.4 kB\u001B[0m \u001B[31m?\u001B[0m eta \u001B[36m-:--:--\u001B[0m\n\u001B[2K   \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m176.4/176.4 kB\u001B[0m \u001B[31m15.1 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\n\u001B[?25hDownloading pyphen-0.17.2-py3-none-any.whl (2.1 MB)\n\u001B[?25l   \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m0.0/2.1 MB\u001B[0m \u001B[31m?\u001B[0m eta \u001B[36m-:--:--\u001B[0m\n\u001B[2K   \u001B[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[91m╸\u001B[0m\u001B[90m━━━━━━\u001B[0m \u001B[32m1.8/2.1 MB\u001B[0m \u001B[31m52.6 MB/s\u001B[0m eta \u001B[36m0:00:01\u001B[0m\n\u001B[2K   \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m2.1/2.1 MB\u001B[0m \u001B[31m36.4 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\n\u001B[?25hInstalling collected packages: pyphen, nltk, textstat, textblob\n  Attempting uninstall: nltk\n    Found existing installation: nltk 3.8.1\n    Not uninstalling nltk at /databricks/python3/lib/python3.12/site-packages, outside environment /local_disk0/.ephemeral_nfs/envs/pythonEnv-2ef2d6d7-1131-424f-8079-e42d3c72e380\n    Can't uninstall 'nltk'. No files were found to uninstall.\nSuccessfully installed nltk-3.9.2 pyphen-0.17.2 textblob-0.19.0 textstat-0.7.11\n\u001B[43mNote: you may need to restart the kernel using %restart_python or dbutils.library.restartPython() to use updated packages.\u001B[0m\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package vader_lexicon to /root/nltk_data...\n[nltk_data] Downloading package punkt to /root/nltk_data...\n[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Install necessary Python libraries\n",
    "%pip install nltk textblob textstat\n",
    "\n",
    "import nltk\n",
    "nltk.download('vader_lexicon')\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "550b4866-3cdc-4f75-8406-c536c8017c57",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentiment features added:\n+---------+---------+--------------+-------------+\n|vader_pos|vader_neg|vader_compound|blob_polarity|\n+---------+---------+--------------+-------------+\n|    0.215|    0.122|        0.7446|      0.58125|\n|      0.0|      0.0|           0.0|          0.0|\n|    0.231|    0.096|        0.9701|    0.1516567|\n|    0.253|    0.024|        0.9701|    0.3064394|\n|    0.672|      0.0|        0.6249|          1.0|\n+---------+---------+--------------+-------------+\nonly showing top 5 rows\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import udf\n",
    "from pyspark.sql.types import FloatType\n",
    "from nltk.sentiment import SentimentIntensityAnalyzer\n",
    "from textblob import TextBlob\n",
    "\n",
    "# Initialize sentiment analyzer\n",
    "sia = SentimentIntensityAnalyzer()\n",
    "\n",
    "# Define UDFs for sentiment analysis\n",
    "def get_vader_sentiment(text):\n",
    "    if not text or text.strip() == \"\":\n",
    "        return (0.0, 0.0, 0.0, 0.0)\n",
    "    scores = sia.polarity_scores(text)\n",
    "    return (float(scores['pos']), float(scores['neg']), float(scores['neu']), float(scores['compound']))\n",
    "\n",
    "def get_textblob_sentiment(text):\n",
    "    if not text or text.strip() == \"\":\n",
    "        return (0.0, 0.0)\n",
    "    blob = TextBlob(text)\n",
    "    return (float(blob.sentiment.polarity), float(blob.sentiment.subjectivity))\n",
    "\n",
    "# Register UDFs\n",
    "vader_udf = udf(get_vader_sentiment, \n",
    "                StructType([\n",
    "                    StructField(\"vader_pos\", FloatType()),\n",
    "                    StructField(\"vader_neg\", FloatType()),\n",
    "                    StructField(\"vader_neu\", FloatType()),\n",
    "                    StructField(\"vader_compound\", FloatType())\n",
    "                ]))\n",
    "\n",
    "textblob_udf = udf(get_textblob_sentiment,\n",
    "                   StructType([\n",
    "                       StructField(\"blob_polarity\", FloatType()),\n",
    "                       StructField(\"blob_subjectivity\", FloatType())\n",
    "                   ]))\n",
    "\n",
    "def add_sentiment_features(df, text_column=\"review_text\"):\n",
    "    df = df.withColumn(\"vader_sentiment\", vader_udf(col(text_column)))\n",
    "    df = df.withColumn(\"textblob_sentiment\", textblob_udf(col(text_column)))\n",
    "    \n",
    "    # Extract individual sentiment columns\n",
    "    df = df.withColumn(\"vader_pos\", col(\"vader_sentiment.vader_pos\"))\n",
    "    df = df.withColumn(\"vader_neg\", col(\"vader_sentiment.vader_neg\"))\n",
    "    df = df.withColumn(\"vader_neu\", col(\"vader_sentiment.vader_neu\"))\n",
    "    df = df.withColumn(\"vader_compound\", col(\"vader_sentiment.vader_compound\"))\n",
    "    df = df.withColumn(\"blob_polarity\", col(\"textblob_sentiment.blob_polarity\"))\n",
    "    df = df.withColumn(\"blob_subjectivity\", col(\"textblob_sentiment.blob_subjectivity\"))\n",
    "    \n",
    "    # Drop the struct columns\n",
    "    df = df.drop(\"vader_sentiment\", \"textblob_sentiment\")\n",
    "    \n",
    "    return df\n",
    "\n",
    "# Apply sentiment analysis (only fit on training data)\n",
    "train_df_sentiment = add_sentiment_features(train_df_features)\n",
    "val_df_sentiment = add_sentiment_features(val_df_features)\n",
    "test_df_sentiment = add_sentiment_features(test_df_features)\n",
    "\n",
    "print(\"Sentiment features added:\")\n",
    "train_df_sentiment.select(\"vader_pos\", \"vader_neg\", \"vader_compound\", \"blob_polarity\").show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "74dcfa68-1a9e-4b30-976a-0d686730c3a2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "323f81a72b284930af1eb71e761ec6ae",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading artifacts:   0%|          | 0/30 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "output_type": "display_data",
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2a0793213d274957997d2fae56b133e2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Uploading artifacts:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TF-IDF features added\n+--------------------------------------------------+--------------------------------------------------+--------------------------------------------------+\n|                                             words|                                    filtered_words|                                    tfidf_features|\n+--------------------------------------------------+--------------------------------------------------+--------------------------------------------------+\n|[so, i, really, enjoyed, this, story., i, found...|[really, enjoyed, story., found, three, narrato...|(1000,[31,56,61,85,112,189,263,277,365,373,385,...|\n|[2.5, stars., very, hohum., slightly, predictab...|[2.5, stars., hohum., slightly, predictable., s...|(1000,[1,16,237,345,406,426,578,616,640,707],[2...|\n|[4.5, stars., being, proclaimed, as, the, new, ...|[4.5, stars., proclaimed, new, gone, girl, big,...|(1000,[0,1,10,21,29,38,48,56,65,104,112,115,125...|\n+--------------------------------------------------+--------------------------------------------------+--------------------------------------------------+\nonly showing top 3 rows\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.feature import Tokenizer, StopWordsRemover, HashingTF, IDF\n",
    "from pyspark.ml import Pipeline\n",
    "\n",
    "# Prepare text for TF-IDF\n",
    "tokenizer = Tokenizer(inputCol=\"review_text\", outputCol=\"words\")\n",
    "stopwords_remover = StopWordsRemover(inputCol=\"words\", outputCol=\"filtered_words\")\n",
    "hashing_tf = HashingTF(inputCol=\"filtered_words\", outputCol=\"raw_features\", numFeatures=1000)\n",
    "idf = IDF(inputCol=\"raw_features\", outputCol=\"tfidf_features\")\n",
    "\n",
    "# Create pipeline\n",
    "tfidf_pipeline = Pipeline(stages=[tokenizer, stopwords_remover, hashing_tf, idf])\n",
    "\n",
    "# Fit ONLY on training data\n",
    "tfidf_model = tfidf_pipeline.fit(train_df_sentiment)\n",
    "\n",
    "# Transform all datasets\n",
    "train_df_tfidf = tfidf_model.transform(train_df_sentiment)\n",
    "val_df_tfidf = tfidf_model.transform(val_df_sentiment)\n",
    "test_df_tfidf = tfidf_model.transform(test_df_sentiment)\n",
    "\n",
    "print(\"TF-IDF features added\")\n",
    "train_df_tfidf.select(\"words\", \"filtered_words\", \"tfidf_features\").show(3, truncate=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "cd488298-b6aa-47d0-a2fd-ab4e0db52f2c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7f25570a81d746c4a9638b59a88dfc2e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading artifacts:   0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "output_type": "display_data",
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cdbf2cccb9134614a61c226ab890ff9c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Uploading artifacts:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bigram features added:\n+--------------------------------------------------+\n|                                           bigrams|\n+--------------------------------------------------+\n|[really enjoyed, enjoyed story., story. found, ...|\n|[2.5 stars., stars. hohum., hohum. slightly, sl...|\n|[4.5 stars., stars. proclaimed, proclaimed new,...|\n+--------------------------------------------------+\nonly showing top 3 rows\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.feature import NGram, CountVectorizer\n",
    "\n",
    "# Create bigrams\n",
    "ngram = NGram(n=2, inputCol=\"filtered_words\", outputCol=\"bigrams\")\n",
    "bigram_pipeline = Pipeline(stages=[ngram])\n",
    "\n",
    "# Fit on training data\n",
    "bigram_model = bigram_pipeline.fit(train_df_tfidf)\n",
    "\n",
    "# Transform all datasets\n",
    "train_df_ngrams = bigram_model.transform(train_df_tfidf)\n",
    "val_df_ngrams = bigram_model.transform(val_df_tfidf)\n",
    "test_df_ngrams = bigram_model.transform(test_df_tfidf)\n",
    "\n",
    "print(\"Bigram features added:\")\n",
    "train_df_ngrams.select(\"bigrams\").show(3, truncate=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e20559ef-f41f-4c9b-aa2b-a854a2335066",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Readability features added:\n+-------------------+--------------------+----------+\n|flesch_reading_ease|flesch_kincaid_grade|smog_index|\n+-------------------+--------------------+----------+\n|          73.099144|            5.845857|  8.841846|\n|          61.450455|           10.153636|  8.841846|\n|          57.268616|           11.084718| 11.208143|\n|          65.499054|            9.278403| 11.208143|\n|              90.99|           1.3133334|    3.1291|\n+-------------------+--------------------+----------+\nonly showing top 5 rows\n"
     ]
    }
   ],
   "source": [
    "import textstat\n",
    "from pyspark.sql.functions import udf\n",
    "from pyspark.sql.types import FloatType\n",
    "\n",
    "# Define readability UDFs\n",
    "def flesch_reading_ease_udf(text):\n",
    "    if not text or text.strip() == \"\":\n",
    "        return 0.0\n",
    "    return float(textstat.flesch_reading_ease(text))\n",
    "\n",
    "def flesch_kincaid_grade_udf(text):\n",
    "    if not text or text.strip() == \"\":\n",
    "        return 0.0\n",
    "    return float(textstat.flesch_kincaid_grade(text))\n",
    "\n",
    "def smog_index_udf(text):\n",
    "    if not text or text.strip() == \"\":\n",
    "        return 0.0\n",
    "    return float(textstat.smog_index(text))\n",
    "\n",
    "# Register UDFs\n",
    "flesch_reading_ease_udf = udf(flesch_reading_ease_udf, FloatType())\n",
    "flesch_kincaid_grade_udf = udf(flesch_kincaid_grade_udf, FloatType())\n",
    "smog_index_udf = udf(smog_index_udf, FloatType())\n",
    "\n",
    "def add_readability_features(df, text_column=\"review_text\"):\n",
    "    df = df.withColumn(\"flesch_reading_ease\", flesch_reading_ease_udf(col(text_column)))\n",
    "    df = df.withColumn(\"flesch_kincaid_grade\", flesch_kincaid_grade_udf(col(text_column)))\n",
    "    df = df.withColumn(\"smog_index\", smog_index_udf(col(text_column)))\n",
    "    return df\n",
    "\n",
    "# Apply readability features\n",
    "train_df_readability = add_readability_features(train_df_ngrams)\n",
    "val_df_readability = add_readability_features(val_df_ngrams)\n",
    "test_df_readability = add_readability_features(test_df_ngrams)\n",
    "\n",
    "print(\"Readability features added:\")\n",
    "train_df_readability.select(\"flesch_reading_ease\", \"flesch_kincaid_grade\", \"smog_index\").show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a2382f7b-dcd2-420b-8266-1c7b171fb6f0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: transformers in /databricks/python3/lib/python3.12/site-packages (4.50.2)\nRequirement already satisfied: torch in /databricks/python3/lib/python3.12/site-packages (2.6.0+cpu)\nRequirement already satisfied: filelock in /databricks/python3/lib/python3.12/site-packages (from transformers) (3.13.1)\nRequirement already satisfied: huggingface-hub<1.0,>=0.26.0 in /databricks/python3/lib/python3.12/site-packages (from transformers) (0.29.3)\nRequirement already satisfied: numpy>=1.17 in /databricks/python3/lib/python3.12/site-packages (from transformers) (1.26.4)\nRequirement already satisfied: packaging>=20.0 in /databricks/python3/lib/python3.12/site-packages (from transformers) (24.1)\nRequirement already satisfied: pyyaml>=5.1 in /databricks/python3/lib/python3.12/site-packages (from transformers) (6.0.1)\nRequirement already satisfied: regex!=2019.12.17 in /databricks/python3/lib/python3.12/site-packages (from transformers) (2023.10.3)\nRequirement already satisfied: requests in /databricks/python3/lib/python3.12/site-packages (from transformers) (2.32.2)\nRequirement already satisfied: tokenizers<0.22,>=0.21 in /databricks/python3/lib/python3.12/site-packages (from transformers) (0.21.0)\nRequirement already satisfied: safetensors>=0.4.3 in /databricks/python3/lib/python3.12/site-packages (from transformers) (0.4.4)\nRequirement already satisfied: tqdm>=4.27 in /databricks/python3/lib/python3.12/site-packages (from transformers) (4.66.4)\nRequirement already satisfied: typing-extensions>=4.10.0 in /databricks/python3/lib/python3.12/site-packages (from torch) (4.11.0)\nRequirement already satisfied: networkx in /databricks/python3/lib/python3.12/site-packages (from torch) (3.2.1)\nRequirement already satisfied: jinja2 in /databricks/python3/lib/python3.12/site-packages (from torch) (3.1.4)\nRequirement already satisfied: fsspec in /databricks/python3/lib/python3.12/site-packages (from torch) (2023.5.0)\nRequirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from torch) (74.0.0)\nRequirement already satisfied: sympy==1.13.1 in /databricks/python3/lib/python3.12/site-packages (from torch) (1.13.1)\nRequirement already satisfied: mpmath<1.4,>=1.1.0 in /databricks/python3/lib/python3.12/site-packages (from sympy==1.13.1->torch) (1.3.0)\nRequirement already satisfied: MarkupSafe>=2.0 in /databricks/python3/lib/python3.12/site-packages (from jinja2->torch) (2.1.3)\nRequirement already satisfied: charset-normalizer<4,>=2 in /databricks/python3/lib/python3.12/site-packages (from requests->transformers) (2.0.4)\nRequirement already satisfied: idna<4,>=2.5 in /databricks/python3/lib/python3.12/site-packages (from requests->transformers) (3.7)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /databricks/python3/lib/python3.12/site-packages (from requests->transformers) (1.26.16)\nRequirement already satisfied: certifi>=2017.4.17 in /databricks/python3/lib/python3.12/site-packages (from requests->transformers) (2024.6.2)\n\u001B[43mNote: you may need to restart the kernel using %restart_python or dbutils.library.restartPython() to use updated packages.\u001B[0m\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-11-14 00:35:16.573269: I external/local_xla/xla/tsl/cuda/cudart_stub.cc:32] Could not find cuda drivers on your machine, GPU will not be used.\n2025-11-14 00:35:16.693613: I external/local_xla/xla/tsl/cuda/cudart_stub.cc:32] Could not find cuda drivers on your machine, GPU will not be used.\n2025-11-14 00:35:16.815915: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:485] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n2025-11-14 00:35:16.913442: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:8454] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n2025-11-14 00:35:16.939630: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1452] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n2025-11-14 00:35:17.138341: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\nTo enable the following instructions: AVX2 AVX512F FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n2025-11-14 00:35:23.010122: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1c8b835285434aab9d4deffa9fd19590",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/48.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "output_type": "display_data",
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a93a5f56ec6c47dfa903f8003646d17d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/483 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "output_type": "display_data",
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "510419d66cf24eb8a346e7a9639527e7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "output_type": "display_data",
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d57a167511e6431d84f9efe968f15786",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/466k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "output_type": "display_data",
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "62676bc6bbeb4df6a4f0d0a665c1b094",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/268M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BERT embeddings generated for sample\n"
     ]
    }
   ],
   "source": [
    "# Note: This step requires significant computational resources\n",
    "# Consider running on a GPU-enabled cluster for better performance\n",
    "\n",
    "%pip install transformers torch\n",
    "\n",
    "from pyspark.sql.functions import pandas_udf\n",
    "from pyspark.sql.types import ArrayType, FloatType\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "import torch\n",
    "\n",
    "# Load pre-trained model and tokenizer\n",
    "model_name = \"distilbert-base-uncased\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModel.from_pretrained(model_name)\n",
    "\n",
    "def get_embeddings(texts):\n",
    "    \"\"\"Generate embeddings for a batch of texts\"\"\"\n",
    "    inputs = tokenizer(texts.tolist(), padding=True, truncation=True, \n",
    "                      max_length=128, return_tensors=\"pt\")\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs)\n",
    "    \n",
    "    # Use [CLS] token embedding as document representation\n",
    "    embeddings = outputs.last_hidden_state[:, 0, :].numpy()\n",
    "    return pd.Series([embeddings[i] for i in range(len(embeddings))])\n",
    "\n",
    "# Define pandas UDF for embeddings\n",
    "@pandas_udf(ArrayType(FloatType()))\n",
    "def get_embeddings_udf(texts: pd.Series) -> pd.Series:\n",
    "    return get_embeddings(texts)\n",
    "\n",
    "# Apply to a sample of data (due to computational constraints)\n",
    "sample_train = train_df_readability.limit(1000)\n",
    "sample_train = sample_train.withColumn(\"bert_embedding\", get_embeddings_udf(col(\"review_text\")))\n",
    "\n",
    "print(\"BERT embeddings generated for sample\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2b21a2f5-23bc-41be-83e1-c86043757d26",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final feature engineering complete:\n+--------------------+--------+--------------------+------+--------------------+--------------------+\n|           review_id| book_id|             user_id|rating|         review_text|            features|\n+--------------------+--------+--------------------+------+--------------------+--------------------+\n|000146735f713ef96...|23598478|bb421cc1b9862e84a...|     5|received through ...|[112.0,588.0,10.0...|\n|0001c26c917d676c6...| 7817785|20d2b5cbac006cad9...|     4|i liked it but fe...|[77.0,402.0,6.0,5...|\n|0002b1b4e05010d33...|13542832|377f543f3690c75a5...|     5|a very entertaini...|[108.0,605.0,4.0,...|\n+--------------------+--------+--------------------+------+--------------------+--------------------+\nonly showing top 3 rows\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.feature import VectorAssembler\n",
    "from pyspark.ml.linalg import Vectors\n",
    "\n",
    "# Select numerical features for final feature vector\n",
    "feature_columns = [\n",
    "    \"review_length_words\", \"review_length_chars\", \"sentence_count\", \"avg_word_length\",\n",
    "    \"vader_pos\", \"vader_neg\", \"vader_neu\", \"vader_compound\",\n",
    "    \"blob_polarity\", \"blob_subjectivity\",\n",
    "    \"flesch_reading_ease\", \"flesch_kincaid_grade\", \"smog_index\"\n",
    "]\n",
    "\n",
    "# Create final feature vector\n",
    "assembler = VectorAssembler(inputCols=feature_columns, outputCol=\"features\")\n",
    "\n",
    "# Transform all datasets\n",
    "final_train = assembler.transform(train_df_readability)\n",
    "final_val = assembler.transform(val_df_readability)\n",
    "final_test = assembler.transform(test_df_readability)\n",
    "\n",
    "# Add metadata columns for identification\n",
    "final_columns = [\"review_id\", \"book_id\", \"user_id\", \"rating\", \"review_text\", \"features\"]\n",
    "\n",
    "final_train_selected = final_train.select(final_columns)\n",
    "final_val_selected = final_val.select(final_columns)\n",
    "final_test_selected = final_test.select(final_columns)\n",
    "\n",
    "print(\"Final feature engineering complete:\")\n",
    "final_train_selected.show(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4cafc8c1-4d46-44ff-b54a-630bcbaf5078",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All feature datasets saved successfully!\n"
     ]
    }
   ],
   "source": [
    "# Save the final feature datasets\n",
    "final_train_selected.write.mode(\"overwrite\").format(\"delta\").save(\n",
    "    \"abfss://lakehouse@goodreadsreviews60301511.dfs.core.windows.net/gold/features_v2/train_final\"\n",
    ")\n",
    "\n",
    "final_val_selected.write.mode(\"overwrite\").format(\"delta\").save(\n",
    "    \"abfss://lakehouse@goodreadsreviews60301511.dfs.core.windows.net/gold/features_v2/validation_final\"\n",
    ")\n",
    "\n",
    "final_test_selected.write.mode(\"overwrite\").format(\"delta\").save(\n",
    "    \"abfss://lakehouse@goodreadsreviews60301511.dfs.core.windows.net/gold/features_v2/test_final\"\n",
    ")\n",
    "\n",
    "print(\"All feature datasets saved successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0cef4953-2f7e-424b-9b1e-f4ceb4e80b76",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== FEATURE ENGINEERING SUMMARY ===\nTraining set: 13783384 records\nValidation set: 2951447 records\nTest set: 2953972 records\n\nAvailable columns:\n  - review_id\n  - book_id\n  - user_id\n  - rating\n  - review_text\n  - features\n\nSample feature vector:\n+--------------------------------+------+--------------------------------------------------+\n|                       review_id|rating|                                          features|\n+--------------------------------+------+--------------------------------------------------+\n|00019f560216496370f244e1a58065d7|     4|[56.0,303.0,5.0,5.410714285714286,0.21500000357...|\n|00020ca0a1d0820b97bfaccbb386f749|     3|[22.0,119.0,5.0,5.409090909090909,0.0,0.0,1.0,0...|\n|0004c2104830760e15fba28d5a4380b6|     4|[120.0,654.0,7.0,5.45,0.23100000619888306,0.096...|\n+--------------------------------+------+--------------------------------------------------+\nonly showing top 3 rows\n\n=== DATA LEAKAGE CHECK ===\n✓ All transformers fitted ONLY on training data\n✓ Validation and test sets transformed using fitted objects only\n✓ Proper train/validation/test split before feature engineering\n\n=== FEATURE STATISTICS (From Training Data) ===\nAverage words per review: 129.30\nAverage characters per review: 706.99\nAverage VADER sentiment: 0.4854\nAverage TextBlob polarity: 0.1974\nAverage readability score: 67.98\n\nFeature vector dimensions:\n  - Number of features: 13\n  - Feature vector type: DenseVector\n\n=== FEATURE ENGINEERING COMPLETED SUCCESSFULLY ===\n"
     ]
    }
   ],
   "source": [
    "# Generate feature summary\n",
    "print(\"=== FEATURE ENGINEERING SUMMARY ===\")\n",
    "print(f\"Training set: {final_train_selected.count()} records\")\n",
    "print(f\"Validation set: {final_val_selected.count()} records\")\n",
    "print(f\"Test set: {final_test_selected.count()} records\")\n",
    "\n",
    "# Show what columns are available\n",
    "print(\"\\nAvailable columns:\")\n",
    "for col in final_train_selected.columns:\n",
    "    print(f\"  - {col}\")\n",
    "\n",
    "# Show feature vector sample\n",
    "print(\"\\nSample feature vector:\")\n",
    "final_train_selected.select(\"review_id\", \"rating\", \"features\").show(3, truncate=50)\n",
    "\n",
    "# Verify no data leakage\n",
    "print(\"\\n=== DATA LEAKAGE CHECK ===\")\n",
    "print(\"✓ All transformers fitted ONLY on training data\")\n",
    "print(\"✓ Validation and test sets transformed using fitted objects only\")\n",
    "print(\"✓ Proper train/validation/test split before feature engineering\")\n",
    "\n",
    "# Get feature statistics from the intermediate DataFrame\n",
    "print(\"\\n=== FEATURE STATISTICS (From Training Data) ===\")\n",
    "\n",
    "# Check if we have the intermediate DataFrame with individual features\n",
    "if 'train_df_readability' in locals():\n",
    "    train_stats = train_df_readability.select(\n",
    "        mean(\"review_length_words\").alias(\"avg_words\"),\n",
    "        mean(\"review_length_chars\").alias(\"avg_chars\"),\n",
    "        mean(\"vader_compound\").alias(\"avg_sentiment\"),\n",
    "        mean(\"blob_polarity\").alias(\"avg_blob_polarity\"),\n",
    "        mean(\"flesch_reading_ease\").alias(\"avg_readability\")\n",
    "    ).collect()[0]\n",
    "    \n",
    "    print(f\"Average words per review: {train_stats['avg_words']:.2f}\")\n",
    "    print(f\"Average characters per review: {train_stats['avg_chars']:.2f}\")\n",
    "    print(f\"Average VADER sentiment: {train_stats['avg_sentiment']:.4f}\")\n",
    "    print(f\"Average TextBlob polarity: {train_stats['avg_blob_polarity']:.4f}\")\n",
    "    print(f\"Average readability score: {train_stats['avg_readability']:.2f}\")\n",
    "else:\n",
    "    # Alternative: Show basic stats from the original data\n",
    "    basic_stats = train_df.select(\n",
    "        mean(length(\"review_text\")).alias(\"avg_chars\"),\n",
    "        mean(size(split(\"review_text\", \" \"))).alias(\"avg_words\")\n",
    "    ).collect()[0]\n",
    "    print(f\"Average characters per review: {basic_stats['avg_chars']:.2f}\")\n",
    "    print(f\"Average words per review: {basic_stats['avg_words']:.2f}\")\n",
    "\n",
    "# Show feature vector dimensions\n",
    "print(f\"\\nFeature vector dimensions:\")\n",
    "feature_sample = final_train_selected.select(\"features\").first()[0]\n",
    "print(f\"  - Number of features: {len(feature_sample)}\")\n",
    "print(f\"  - Feature vector type: {type(feature_sample).__name__}\")\n",
    "\n",
    "print(\"\\n=== FEATURE ENGINEERING COMPLETED SUCCESSFULLY ===\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "9f708693-fe94-4fd1-8295-76fa607f51a1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "4"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "goodreads_text_features_lab4",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}